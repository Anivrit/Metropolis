{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "random.seed(42)\n",
    "\n",
    "input_dim=784\n",
    "hidden=100\n",
    "global M_1\n",
    "M_1 = np.eye(hidden,input_dim)\n",
    "global M_2\n",
    "M_2 = np.eye(10,hidden)\n",
    "T = 40000\n",
    "T_2=10 #inner level running time\n",
    "s_prop_1=0.001 #Variance of proposal distribution q_1\n",
    "s_prop_2=0.0005 #Variance of proposal distribution q_2\n",
    "\n",
    "#Temperature vector:\n",
    "a_1=2*10**(-6)\n",
    "a_2=10**(-6)\n",
    "\n",
    "\n",
    "Hist_train = np.zeros(T) #Training errors\n",
    "Hist_test = np.zeros(T) #Testing errors\n",
    "\n",
    "#initialization\n",
    "W_1 = np.zeros((hidden,input_dim))\n",
    "W_2 = np.zeros((10,hidden))\n",
    "\n",
    "def loss(w1,w2):\n",
    "    global M_1\n",
    "    global M_2\n",
    "    lost=np.exp(-neural_net_loss(data.train_x, data.train_y_one_hot,w1+M_1,w2+M_2));\n",
    "    return lost\n",
    "\n",
    "def neural_net_loss(input_instance, input_label,V_1,V_2):\n",
    "    x_1 = np.maximum((V_1@input_instance),np.zeros((np.shape(V_1@input_instance))))\n",
    "    x_2 = V_2@x_1\n",
    "    Net_output=x_2\n",
    "    for i in range(np.max(np.shape(Net_output))):\n",
    "        temp = 0\n",
    "        for k in range(np.min(np.shape(Net_output))):\n",
    "            temp += np.exp(k)\n",
    "        for j in range(np.min(np.shape(Net_output))):\n",
    "            Net_output[j][i] = np.exp(x_2[j][i])/temp\n",
    "    Loss_output = np.sum(np.square(input_label-Net_output),axis=0)\n",
    "    loss_amount = np.mean(Loss_output)\n",
    "    return loss_amount\n",
    "\n",
    "\n",
    "for t in range(T):\n",
    "    print(t)\n",
    "    num_A=0\n",
    "    W_1_hat = W_1 + np.random.normal(0,s_prop_1,size = (hidden,input_dim))\n",
    "    for u in range(T_2):\n",
    "        W_2_hat = W_2 + np.random.normal(0,s_prop_2,size = (10,hidden))\n",
    "        b=(loss(W_1,W_2_hat))/(loss(W_1,W_2)**(1/a_2))\n",
    "        v = random.random()\n",
    "        if(v<b):\n",
    "            W_2 = W_2_hat\n",
    "            print(\"W_2 Changed\")\n",
    "        num_A=num_A+(loss(W_1_hat,W_2))/(loss(W_1,W_2)**(1/a_2));\n",
    "    norm_W_1 = np.linalg.norm(W_1)\n",
    "    norm_W_2 = np.linalg.norm(W_2)\n",
    "    train_error=neural_net_loss(data.train_x, data.train_y_one_hot,W_1+M_1,W_2+M_2)\n",
    "    test_error=neural_net_loss(data.test_x, data.test_y_one_hot,W_1+M_1,W_2+M_2)\n",
    "    Hist_train[t]=train_error\n",
    "    Hist_test[t]=test_error\n",
    "    A=num_A/(T_2)\n",
    "    C=A**(a_2/(a_1+a_2))\n",
    "    w = random.random()\n",
    "    if(w<C):\n",
    "        W_1=W_1_hat\n",
    "        print('W_1 Changed')\n",
    "\n",
    "#Next Step is to Plot data\n",
    "\n",
    "plt.plot(range(T),Hist_train)\n",
    "plt.plot(range(T),Hist_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
